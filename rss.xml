<rss version="2.0"><channel><title>gallais&#39; blog</title><link>http://blog.gallais.org</link><description>gallais&#39; blog</description><item><title>Cyclic Lists, Purely</title><link>http://www.gallais.org/blog/cyclic-list-purely.html</link><description>&lt;head&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot;       href=&quot;http://www.gallais.org/css/main.css&quot; /&gt;&lt;/head&gt;&lt;span style=&quot;float:right&quot;&gt;&lt;a href=&quot;/rss.xml&quot;&gt;&lt;img src=&quot;/img/rss.png&quot; /&gt;&lt;/a&gt;&lt;/span&gt;&lt;div class=bdocs&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/gallais/potpourri/blob/master/haskell/cyclic/CyclicList.hs&quot;&gt;Haskell code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.citeulike.org/user/gallais/article/1244296&quot;&gt;Fegaras &amp; Sheard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.citeulike.org/user/gallais/article/8246712&quot;&gt;Ghani et al.&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;


&lt;p id=chapo&gt;In the paper
&lt;a href=&quot;http://www.citeulike.org/user/gallais/article/1244296&quot;&gt;
&lt;i&gt;Representing Cyclic Structures as Nested Datatypes&lt;/i&gt;&lt;/a&gt;,
the authors dismiss quite quickly the idea of encoding cyclic lists
by using a function space to represent the loop in favour of
(type-level) de Bruijn levels. I wanted to see how far one can go
with the more extensional approach whilst retaining good properties
(canonical forms, productivity, etc.). It turns out that the answer
is: &quot;quite far, really&quot;.&lt;/p&gt;



&lt;p&gt;The setup is pretty simple: we want to represent cyclic lists in
a purely functional language. A naïve solution would be to turn to
codata and use streams built using a fixpoint in Haskell. However,
representing the cycle with a material artefact will allow us to
analyze it (is it finite?), display it as a string by writing the
fixpoint down explicitly, make serialization trivial or transform
it by mapping a function, permuting the elements, or adding /
removing some of them from the cycle.&lt;/p&gt;



&lt;h3&gt;Fegaras and Sheard&#39;s solution&lt;/h3&gt;



&lt;p&gt;Cyclic lists are represented using a mixed-variant datatype. On
top of the expected &lt;tt&gt;CNil&lt;/tt&gt; and &lt;tt&gt;Cons&lt;/tt&gt; shared with usual
lists, a &lt;tt&gt;CRec&lt;/tt&gt; constructor is provided. It declares a pointer
which may be used later on to represent that very node. It should be
noted that &lt;tt&gt;CRec&lt;/tt&gt; is &lt;tt&gt;Cons&lt;/tt&gt;-like in order to guarantee
that all cycles are productive.&lt;/p&gt;



&lt;p class=code&gt;data CList a = CNil
             | Cons a (CList a)
             | CRec a (CList a -&gt; CList a)&lt;/p&gt;



&lt;p&gt;This definition exploits the function space of the host language
to handle the pointer declaration in a sort of Higher Order
Abstract Syntax fashion. It makes giving a semantics for it in terms
of streams quite easy: one just needs to feed the function stored in
&lt;tt&gt;CRec&lt;/tt&gt; with the list itself.&lt;/p&gt;



&lt;p class=code&gt;toStream :: CList a -&gt; [a]
toStream CNil          = []
toStream (Cons x xs)   = x : toStream xs
toStream xs@(CRec x r) = x : toStream (r xs)&lt;/p&gt;



&lt;h3&gt;The limitations of this representation&lt;/h3&gt;



&lt;p&gt;Now, this representation has multiple problems. They are pointed
out in Ghani et al. to justify their favouring of a more intensional
approach to cycle representation. Most of them relate to the fact
that it does not put enough restrictions on the ways one may use
the variable provided by the &lt;tt&gt;CRec&lt;/tt&gt; constructor. As a
consequence, the encoding lacks canonicity.&lt;/p&gt;



&lt;h3&gt;There is no guarantee on the way pointers are used (if at all)&lt;/h3&gt;



&lt;p&gt;Given that the cycle is represented using a function space, it
is hard to detect whether the variable has been used at all, or if
it has been used whether it was in a proper way or not. So far
nothing prevents a list from pretending to be cyclic by introducing
a pointer it will never use:&lt;/p&gt;



&lt;p class=code&gt;finite = Cons 1 $ CRec 2 $ const CNil&lt;/p&gt;



&lt;p&gt;Or introduce multiple pointers only one of which is meant to be
declared whilst the other ones are &lt;tt&gt;CRec&lt;/tt&gt; cells misused as
&lt;tt&gt;Cons&lt;/tt&gt; ones. This leads to the possibility of different
representations of the same object:&lt;/p&gt;



&lt;p class=code&gt;fourTwos = Cons 4 $ CRec 2 id
arghTwos = CRec 4 $ const $ CRec 2 id&lt;/p&gt;



&lt;p&gt;Last but not least, these lists can also fail at being cyclic
because a list transformer has been used improperly in the body
of the cycle without being rejected by the typechecker:&lt;/p&gt;



&lt;p class=code&gt;nats = CRec 0 $ cmap (+1)&lt;/p&gt;



&lt;h3&gt;Useful functions cannot be written without unfolding the
cycle&lt;/h3&gt;



&lt;p&gt;But there is more! Even if one is willing to excuse these
imperfections, it is hard to define basic functions without
unwinding the cycle. It is for instance impossible to map a
function across a list without unfoding it. Indeed, using a
&lt;tt&gt;CRec&lt;/tt&gt; constructor in the last case would provide us
with a &lt;tt&gt;CList b&lt;/tt&gt; element but &lt;tt&gt;r&lt;/tt&gt; expects a
&lt;tt&gt;CList a&lt;/tt&gt; one.&lt;/p&gt;



&lt;p class=code&gt;cmap :: (a -&gt; b) -&gt; CList a -&gt; CList b
cmap f CNil          = CNil
cmap f (Cons x xs)   = Cons (f x) $ cmap f xs
&lt;span class=bad&gt;cmap f xs@(CRec x r) = Cons (f x) $ cmap f (r xs)&lt;/span&gt;&lt;/p&gt;



&lt;p&gt;At that point, the picture we have drawn is pretty grim and we
can understand why one may want to ditch the extensional approach
altogether. But it turns out that there is a solution to all of our
problems: the type system!&lt;/p&gt;



&lt;h3&gt;Calling the type system to the rescue.&lt;/h3&gt;



&lt;p&gt;A few days before being confronted with this issue, I had read
the very interesting &lt;a href=&quot;https://tel.github.io/2014/07/15/mutable_algorithms_in_immutable_languages_part_3/&quot;&gt;account of the ST monad by Joseph Abrahamson&lt;/a&gt;
explaining how the implementation used rank 2 polymorphism to avoid
references escaping their environment. It was only natural to recycle
that knowledge in our case to avoid all the API abuses evoked in the
previous section. The new &lt;tt&gt;CList&lt;/tt&gt; type constructor takes two
arguments; the first one is the type of the elements it contains
whilst the second one is a phantom type used to enforce that the
constructor are only ever used in the way they were meant to.&lt;/p&gt;



&lt;p class=code&gt;data CList a b where
  CNil :: CList a Closed
  Cons :: a -&gt; CList a b -&gt; CList a b
  CRec :: a -&gt; (forall b. CList a b -&gt; CList a b) -&gt; CList a Closed&lt;/p&gt;



&lt;p&gt;&lt;tt&gt;CRec&lt;/tt&gt;&#39;s argument has to be polymorphic in the phantom
type which means that it may not use &lt;tt&gt;CRec&lt;/tt&gt; or &lt;tt&gt;CNil&lt;/tt&gt;
both of which fix the phantom type to &lt;tt&gt;Closed&lt;/tt&gt;. In effect,
it has to be a finite number of &lt;tt&gt;Cons&lt;/tt&gt; followed by the
variable provided. In other words, an element of type
&lt;tt&gt;CList a Closed&lt;/tt&gt; describes precisely a potentially cyclic
list &lt;a id=&quot;reftop1&quot; href=&quot;#refbot1&quot;&gt;[1]&lt;/a&gt;. Hence the following type alias:&lt;/p&gt;



&lt;p class=code&gt;data Closed = Closed
type List a = CList a Closed&lt;/p&gt;



&lt;p&gt;All the broken examples are now rightfully rejected by the type
system which detects that we are trying to conflate types which are
incompatible when we declare a pointer but forget to use it, when we
declare multiple ones, etc. That is already a nice change but we
are still facing a major challenge: writing functions such as
&lt;tt&gt;map&lt;/tt&gt; without unfolding the cycle. This is what Ghani et al.
write on that topic:&lt;/p&gt;



&lt;p class=quote&gt; &quot;Although cycles are explicit in this representation,
almost all functions manipulating cyclic structures will need to
unwind the cycles (based on the intuition that Rec means fix). The
reason is that, although, we can recognize cycles as structures of
the form Rec f, the argument f is a function and we cannot analyze
functions well&quot;.&lt;/p&gt;



&lt;p&gt;This was absolutely true in the situation they were facing. But
by exploiting the type system, we have managed to root out an
awful lot of unwanted constructs. To the point that we now know
for sure that there is &lt;i&gt;at most one&lt;/i&gt; &lt;tt&gt;CRec&lt;/tt&gt; constructor
in a &lt;tt&gt;List a&lt;/tt&gt;. This implies in particular that we can detect
where the pointer declared by a &lt;tt&gt;CRec&lt;/tt&gt; is used. Indeed, if
&lt;tt&gt;xs&lt;/tt&gt; is equal to &lt;tt&gt;CRec x f&lt;/tt&gt; then we know for sure that
&lt;tt&gt;f xs&lt;/tt&gt; will contain a &lt;tt&gt;CRec&lt;/tt&gt; and that it will be the
head constructor of &lt;tt&gt;xs&lt;/tt&gt; showing up where the pointer was
used.&lt;/p&gt;



&lt;p&gt;This means that we can actually build a &lt;tt&gt;cfold&lt;/tt&gt; function
which will not unwind the cycle unless we want it to. Let&#39;s start
by writing down the most general type for it. The two first arguments
corresponds to the usual &lt;tt&gt;Cons&lt;/tt&gt; and &lt;tt&gt;CNil&lt;/tt&gt; cases with
the right phantom types sprinkled where necessary. The third one
is the one dealing with cycles and it mimics really closely the type
of &lt;tt&gt;CRec&lt;/tt&gt;. The whole thing is implemented in terms of an
auxiliary function.&lt;/p&gt;



&lt;p class=code&gt;cfold :: forall a (b :: * -&gt; *).
         (forall ph. a -&gt; b ph -&gt; b ph) -&gt;
         b Closed -&gt;
         (a -&gt; (forall ph. b ph -&gt; b ph) -&gt; b Closed) -&gt;
         List a -&gt; b Closed
cfold c n r = goCRec&lt;/p&gt;



&lt;p&gt;This auxiliary function unfolds the list argument applying the
appropriate combinator to the constructor&#39;s arguments and the
the induction hypothesis. As its name &lt;tt&gt;goCRec&lt;/tt&gt; suggests, it
traverses the &lt;tt&gt;CRec&lt;/tt&gt; constructor before handing over the rest
of the work to &lt;tt&gt;stopCRec&lt;/tt&gt;.&lt;/p&gt;



&lt;p class=code&gt;goCRec :: forall ph. CList a ph -&gt; b ph
goCRec CNil          = n
goCRec (Cons x xs)   = c x $ goCRec xs
goCRec xs@(CRec x p) = r x $ stopCRec (p xs)&lt;/p&gt;



&lt;p&gt;&lt;tt&gt;stopCRec&lt;/tt&gt; is similarly structurally working its way
through the list except that it posesses an induction hypothesis
&lt;tt&gt;ih&lt;/tt&gt; obtained earlier on by &lt;tt&gt;goCRec&lt;/tt&gt;. When faced with
a &lt;tt&gt;CRec&lt;/tt&gt;, we know for sure that we have just followed the
pointer back to where it was declared. As a consequence, we return
the induction hypothesis rather than unfolding the cycle yet
another time.&lt;/p&gt;



&lt;p class=code&gt;stopCRec :: CList a Closed -&gt; forall ph. b ph -&gt; b ph
stopCRec (Cons x xs) ih = c x $ stopCRec xs ih
stopCRec (CRec _ _)  ih = ih&lt;/p&gt;



&lt;p&gt;Now that we have this induction principle, it is really easy to
define a &lt;tt&gt;cmap&lt;/tt&gt; which outputs a list with a structure
identical to the one we started with.&lt;/p&gt;



&lt;p class=code&gt;cmap :: forall a b. (a -&gt; b) -&gt; List a -&gt; List b
cmap f = cfold (Cons . f) CNil (CRec . f)&lt;/p&gt;



&lt;p&gt;And indeed, &lt;tt&gt;cmap (+1) $ CRec 1 (Cons 2)&lt;/tt&gt; evaluates down to
&lt;tt&gt;CRec 2 (Cons 3)&lt;/tt&gt; rather than the detestable
&lt;tt&gt;Cons 2 (Cons 3 (Cons 2 (...)))&lt;/tt&gt; the naïve solution used to
produce.&lt;/p&gt;



&lt;p&gt;It is also possible to instantiate this very generic scheme down
to ones with simpler types. One such instance is the induction
principle where the return type does not depend on the phantom
type:&lt;/p&gt;



&lt;p class=code&gt;cfoldRec :: forall a b. (a -&gt; b -&gt; b) -&gt; b -&gt;
            (a -&gt; (b -&gt; b) -&gt; b) -&gt; List a -&gt; b&lt;/p&gt;



&lt;p&gt;An elegant use case of this &lt;tt&gt;cfoldRec&lt;/tt&gt; is the definition
of the &lt;tt&gt;Show&lt;/tt&gt; instance for &lt;tt&gt;List a&lt;/tt&gt; which allows us
to display a finite string representing the cyclic structure in its
entirety. When we encounter the pointer declaration, we output
&quot;rec X. &quot; and keep showing the rest of the list but when the pointer
is used, we simply output &quot;X&quot; and stop.&lt;/p&gt;



&lt;p class=code&gt;instance Show a =&gt; Show (List a) where
  show = cfoldRec (\ x -&gt; (++) (show x ++ &quot; : &quot;)) &quot;[]&quot;
                  (\ x ih -&gt; &quot;rec X. &quot; ++ show x ++ &quot; : &quot; ++ ih &quot;X&quot;)&lt;/p&gt;



&lt;p&gt;And, as we expected, &lt;tt&gt;show $ CRec 1 (Cons 2)&lt;/tt&gt; now reduces
to the finite string &lt;tt&gt;&quot;rec X. 1 : 2 : X&quot;&lt;/tt&gt;. It is also possible
to recover a &lt;tt&gt;fold&lt;/tt&gt;-style function unwinding the cycle by
tying the knot in the third argument: the induction hypothesis
will just be the whole subcomputation itself!&lt;/p&gt;



&lt;p class=code&gt;cfold&#39; :: forall a b. (a -&gt; b -&gt; b) -&gt; b -&gt; List a -&gt; b
cfold&#39; c n = cfoldRec c n r
  where r :: a -&gt; (b -&gt; b) -&gt; b
        &lt;span class=bad&gt;r a ih&lt;/span&gt; = c a (ih $ &lt;span class=bad&gt;r a ih&lt;/span&gt;)&lt;/p&gt;



&lt;p&gt;This is precisely what we want to do when defining the semantics
of these cyclic lists in terms of streams. Hence the really concise
definition:&lt;/p&gt;



&lt;p class=code&gt;toStream :: List a -&gt; [ a ]
toStream = cfold&#39; (:) []&lt;/p&gt;



&lt;p&gt;There are still plenty of functions to implement on this
representation some of which may raise interesting questions:
&lt;tt&gt;czip&lt;/tt&gt; will surely involve computing a least common
multiple. And it will enable us to define an &lt;i&gt;extensional&lt;/i&gt;
equality on these cyclic structures. Exciting perspectives.&lt;/p&gt;



&lt;h3&gt;Conclusion&lt;/h3&gt;



&lt;p&gt;GADTs combined with higher rank polymorphism allowed us to bake
in invariants which were sufficient for us to write more useful
functions than the rather untyped approach led us to believe was
possible. The type of &lt;tt&gt;cfold&lt;/tt&gt; lets us decide at any time
whether we want to unwind the cycle or not.&lt;/p&gt;



&lt;p&gt;However, this approach is limited to cases in which it only
makes sense to have &lt;i&gt;one&lt;/i&gt; cycle. This is not true anymore
when we consider datastructures which are not linear: a cyclic
tree may have different cycle in different branches, some of
which may be nested. It is then necessary to fall back to a
setup managing syntax with binding.&lt;/p&gt;


&lt;h3&gt;Footnotes&lt;/h3&gt;&lt;a id=&quot;refbot1&quot; href=&quot;#reftop1&quot;&gt;[1]&lt;/a&gt; We consider here that we only work in the inductive
fragment of the language. It is obviously impossible to enforce in
Haskell.</description><guid isPermaLink="true">http://www.gallais.org/blog/cyclic-list-purely.html</guid><pubDate>Tue, 12 Aug 2014 13:37:00 UTC</pubDate></item><item><title>Non-regular Parameters are OK</title><link>http://www.gallais.org/blog/non-regular-parameters.html</link><description>&lt;head&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot;       href=&quot;http://www.gallais.org/css/main.css&quot; /&gt;&lt;/head&gt;&lt;span style=&quot;float:right&quot;&gt;&lt;a href=&quot;/rss.xml&quot;&gt;&lt;img src=&quot;/img/rss.png&quot; /&gt;&lt;/a&gt;&lt;/span&gt;&lt;div class=bdocs&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://www.lix.polytechnique.fr/~barras/habilitation/&quot;&gt;Barras&#39; habil. thesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;code/NonRegularParameter/NonRegular.agda&quot;&gt;Raw Agda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;code/NonRegularParameter/NonRegular.html&quot;&gt;Colored Agda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;code/NonRegularParameter/NonRegular.v&quot;&gt;Coq equivalent&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;


&lt;p id=chapo&gt;Both Coq and Agda let the user declare datatypes with
non-regular parameters which do vary, are not quite indices and
can be large without bumping the universe level in which the
declaration lives. In his habilitation thesis, Bruno Barras
shows a nice trick explaining why these non-regular parameters
make sense.&lt;/p&gt;




&lt;h3&gt;Quick definition&lt;/h3&gt;



&lt;p&gt;A non-regular parameter for an inductive type is an (usually large) argument
of the &lt;i&gt;type&lt;/i&gt; constructor which is used uniformly in the return type of its
&lt;i&gt;term&lt;/i&gt; constructors (thus making it parameter-like) but may vary in their
recursive positions (making it index-like).&lt;/p&gt;



&lt;p&gt;First of all, let&#39;s see a couple of use cases for non-regular parameters.

In Ralf Hinze and Ross Paterson&#39;s
&lt;a href=&quot;http://hackage.haskell.org/package/fingertree&quot;&gt;fingertrees&lt;/a&gt;, the
non regular parameter « determines the unusual shape of these trees, which is
the key to their performance »
&lt;a id=&quot;reftop1&quot; href=&quot;#refbot1&quot;&gt;[1]&lt;/a&gt;. The non regularity is here used to enforce invariants
in the substructures by tightly controlling which constructors one may use.&lt;/p&gt;



&lt;p class=code&gt;data FingerTree a = Empty
                  | Single a
                  | Deep (Digit a) (FingerTree (Node a)) (Digit a)
&lt;/p&gt;



&lt;p&gt;Using &lt;a href=&quot;?en/main/blog/read/lazy-lambda#section2&quot;&gt;Type-level de Bruijn
indices&lt;/a&gt; in order to bake in a syntax a notion of scope and binding sites
is another use of non regularity. In this case, the invariant we are enforcing
is the fact that one can only talk about variables which are in scope. But it
also formalizes lets us identify clearly the constructors which are binders.&lt;/p&gt;



&lt;p&gt;Now that we have seen that this non-regularity can be useful, we would like
to know whether it is a safe feature. After all, having a system in which
&lt;tt&gt;Type : Type&lt;/tt&gt; can simplify things quite a lot but it comes at the cost
of being inconsistent...&lt;/p&gt;



&lt;h3&gt;The encoding&lt;/h3&gt;



&lt;p&gt;We will not give a generic account of this transformation but explain
it by a simple example. Let&#39;s use the type-level de Bruijn indices case
to demonstrate how one can explain the use of a non-regular parameter in
terms of more basic concepts which are known to be sound. We will work in
Agda but it would be possible to perform the same construction in Coq as
demonstrated by &lt;a href=&quot;code/NonRegularParameter/NonRegularParameter.v&quot;&gt;this
file&lt;/a&gt;.&lt;/p&gt;



&lt;h4&gt;What one would write in Agda&lt;/h4&gt;



&lt;p&gt;We start by defining our simple lambda calculus &lt;tt&gt;Lam&lt;/tt&gt; equipped
with a let binder. The parameter &lt;tt&gt;A&lt;/tt&gt; represents the variables available
in scope; it needs therefore to be modified every time a binder introduces
fresh ones &lt;a id=&quot;reftop2&quot; href=&quot;#refbot2&quot;&gt;[2]&lt;/a&gt;:&lt;/p&gt;



&lt;p class=code&gt;data Lam (A : Set) : Set where
  `var      : (a : A) → Lam A
  `app      : (t u : Lam A) → Lam A
  `lam      : (t : Lam (1 + A)) → Lam A
  `let_`in_ : {n : ℕ} (t : Vec (Lam A) n) (u : Lam (n + A)) → Lam A
&lt;/p&gt;



&lt;h4&gt;A naïve translation using large indexes&lt;/h4&gt;



&lt;p&gt;Given that this argument varies in the various recursive positions,
one could consider calling it an index. This is where we run into troubles:
the value &lt;tt&gt;A&lt;/tt&gt; used to be in context because it was treated like a
parameter; now that we consider it to be an index, every constructor has to
introduce &lt;tt&gt;A&lt;/tt&gt; to be able to mention it. But &lt;tt&gt;A&lt;/tt&gt; is large which
forces the datatype declaration to live one level up.&lt;/p&gt;



&lt;p class=code&gt;data Lam : (A : Set) → Set₁ where
  `var      : {A : Set} (a : A) → Lam A
  `app      : {A : Set} (t u : Lam A) → Lam A
  `lam      : {A : Set} (t : Lam (1 + A)) → Lam A
  `let_`in_ : {A : Set} {n : ℕ} (t : Vec (Lam A) n) (u : Lam (n + A)) → Lam A&lt;/p&gt;



&lt;p&gt;This is quite clearly not a satisfactory solution. It is, for instance, now
impossible to define the monadic &lt;tt&gt;join&lt;/tt&gt; for &lt;tt&gt;Lam&lt;/tt&gt; because
&lt;tt&gt;Lam (Lam A)&lt;/tt&gt; is not even a type one can form! If we do need to introduce
an index to track down the modifications made to the non regular parameter then
it&#39;d better be a small one. This is precisely what the encoding does.&lt;/p&gt;



&lt;h4&gt;Barras&#39; encoding&lt;/h4&gt;



&lt;p&gt;The important remark behind this encoding is that the shape of
the non-regular parameter at any point in a tree is entirely determined
by the value of the parameter at the root; the path followed from the root
down to this subtree and, potentially, additional information stored in
the nodes encountered along this path. Now, this information has to be small,
otherwise the inductive definition would not fit at its current level. It is
therefore sufficient to store, as a parameter, the value present at the root
and then use a &lt;i&gt;small&lt;/i&gt; index to describe the path followed and the
information accumulated. In the case of our running example, the type of path
is as follows.&lt;/p&gt;



&lt;p class=code&gt;data path : Set where
  `rt : path
  `bd : (n : ℕ) (p : path) → path&lt;/p&gt;



&lt;p&gt;&lt;tt&gt;`rt&lt;/tt&gt; represents the root of the tree and &lt;tt&gt;`bd&lt;/tt&gt; (for &quot;binder&quot;)
represents both &lt;tt&gt;`lam&lt;/tt&gt; and &lt;tt&gt;`let_`in_&lt;/tt&gt; because we are only
interested in the number of variables introduced by a binding site. We note
that one can blithely forget about the &lt;tt&gt;`app&lt;/tt&gt; constructors one crossed
because they do not alter the parameter. The &lt;tt&gt;decode&lt;/tt&gt; function defining
the semantics of these paths is straightforward: the root induces the identity
and a binding site extends the environment with &lt;tt&gt;n&lt;/tt&gt; new variables.&lt;/p&gt;



&lt;p class=code&gt;decode : (p : path) (A : Set) → Set
decode `rt       A = A
decode (`bd n p) A = n + decode A p&lt;/p&gt;



&lt;p&gt;This allows us to define a datatype in a theory with only parameters and
indices whilst keeping it small. As one would expect, it is possible to prove
that this inductive type is in bijection with the original one. The usual
problem arise with the termination checker when dealing with the nesting
&lt;tt&gt;Vec (Lam A) n&lt;/tt&gt; but, as usual, inlining &lt;tt&gt;Vec.map&lt;/tt&gt; is sufficient
to convince Agda that everything is fine.&lt;/p&gt;



&lt;p class=code&gt;data Lam (A : Set) : path → Set where
  `var      : {p : path} (a : decode p A) → Lam A p
  `app      : {p : path} (t u : Lam A p) → Lam A p
  `lam      : {p : path} (t : Lam A (`bd 1 p)) → Lam A p
  `let_`in_ : {p : path} {n : ℕ} (t : Vec (Lam A p) n) (u : Lam A (`bd n p)) →
              Lam A p&lt;/p&gt;



&lt;h3&gt;Conclusion&lt;/h3&gt;



&lt;p&gt;Well that was fun! If you want a more general presentation, then have a look
at &lt;a href=&quot;http://www.lix.polytechnique.fr/~barras/habilitation/&quot;&gt;the thesis&lt;/a&gt;
page 138 and following. Beware though: I started reading it because it was
mentioned somewhere that it explained how to make sense, in terms of indexed
families, of mutually defined inductive types leaving at different levels;
and stayed because I kept seeing other interesting topics. It&#39;s a trap.&lt;/p&gt;

&lt;h3&gt;Footnotes&lt;/h3&gt;&lt;a id=&quot;refbot1&quot; href=&quot;#reftop1&quot;&gt;[1]&lt;/a&gt; Page 4, &lt;a href=&quot;http://www.citeulike.org/user/gallais/article/2399086&quot;&gt;
Finger trees: a simple general-purpose data structure&lt;/a&gt; by Ralf Hinze, Ross
Paterson.&lt;br /&gt;&lt;a id=&quot;refbot2&quot; href=&quot;#reftop2&quot;&gt;[2]&lt;/a&gt; &lt;tt&gt;_+_&lt;/tt&gt; of type &lt;tt&gt;ℕ → Set → Set&lt;/tt&gt; is used here
to mean that a finite number of new bound variable have been introduced by the
binding construct.</description><guid isPermaLink="true">http://www.gallais.org/blog/non-regular-parameters.html</guid><pubDate>Thu, 17 Jul 2014 13:37:00 UTC</pubDate></item><item><title>Lazy Weakening and Equality Test</title><link>http://www.gallais.org/blog/lazy-lambda.html</link><description>&lt;head&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot;       href=&quot;http://www.gallais.org/css/main.css&quot; /&gt;&lt;/head&gt;&lt;span style=&quot;float:right&quot;&gt;&lt;a href=&quot;/rss.xml&quot;&gt;&lt;img src=&quot;/img/rss.png&quot; /&gt;&lt;/a&gt;&lt;/span&gt;&lt;div class=bdocs&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.fpcomplete.com/user/edwardk/bound&quot;&gt;Intro to bound&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://hackage.haskell.org/package/bound&quot;&gt;bound on Hackage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;code/LazyLambda/LazyLambda.html&quot;&gt;Colored Agda&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;


&lt;p id=chapo&gt;One of the key features of &lt;tt&gt;bound&lt;/tt&gt; is that it
introduces redundancies in the representation of syntaxes in
order to avoid traversing entire subterms just to apply a
weakening. However, equality testing currently quotients out
these redundancies. In this post, I present a lazy equality
test for the untyped lambda calculus with redundancies which does
not perform any of this extra work.&lt;/p&gt;



&lt;p&gt;This works stems out of a bold statement I made and was called
out on by Thomas Braibant during an emacs-driven seminar given in
front of the smart &lt;a href=&quot;http://gallium.inria.fr/blog/&quot;&gt;gallium&lt;/a&gt;
people. The lazy equality test I claimed could be implemented turned
out to be quite a bit more work than I expected &amp; I was not able to
deliver it when asked to. Better late than never.&lt;/p&gt;



&lt;p&gt;Our running example will be the untyped lambda calculus
with its well-known 3 constructors: &lt;tt&gt;var&lt;/tt&gt;, &lt;tt&gt;lam&lt;/tt&gt; and
&lt;tt&gt;app&lt;/tt&gt;. We will start by motivating the use of the notion
of &lt;tt&gt;Scope&lt;/tt&gt; when dealing with syntaxes with binding both for
its added safety, genericity and efficiency. We will then move on
to the actual meat of this post: the lazy equality test.&lt;/p&gt;



&lt;h3&gt;&lt;a href=&quot;code/LazyLambda/LazyLambda.html#804&quot;&gt;Binding? de Bruijn!&lt;/a&gt;&lt;/h3&gt;



&lt;p&gt;De Bruijn indices are the canonical solution to representing
binding in a language whilst not having to bother with α-conversion.
Here is the inductive definition of the untyped lambda calculus:&lt;/p&gt;



&lt;p class=code&gt;data Lam : Set where
  var : (n : Nat)   → Lam
  app : (t u : Lam) → Lam
  lam : (b : Lam)   → Lam&lt;/p&gt;



&lt;p&gt;Unfortunately this ease of use comes at a price: de Bruijn
indices are an inherently untyped approach to context representation
which means that they don&#39;t push any sanity check into the type
system. Being the silly programmer I am, I always end up writing
nonsensical expressions. The canonical example may very well be the
definition of substitution
&lt;a id=&quot;reftop1&quot; href=&quot;#refbot1&quot;&gt;[1]&lt;/a&gt; where one goes under
a binder without even realizing that the former substitution
must be weakened and extended:&lt;/p&gt;



&lt;p class=code&gt;subst : Lam → (Nat → Lam) → Lam
subst (var x)   ρ = ρ x
subst (app t u) ρ = app (subst t ρ) (subst u ρ)
&lt;span class=bad&gt;subst (lam b)   ρ = lam (subst b ρ)&lt;/span&gt; -- this typechecks...
                                    -- ouch!&lt;/p&gt;



&lt;h3&gt;&lt;a href=&quot;code/LazyLambda/LazyLambda.html#2195&quot;&gt;Type level de Bruijn indices&lt;/a&gt;&lt;/h3&gt;



&lt;p&gt;Now, de Bruijn indices are a really great idea when it comes
to handling α-equivalence, capture avoidance, looking variables up
in environments, etc. We would like to retain the general idea behind
them whilst being reminded by the typechecker that some magic needs
to happen after a new variable has been bound.&lt;/p&gt;



&lt;p&gt;Typed alternatives to de Bruijn indices do exist; we can e.g.
think of terms well-scoped by constructions using &lt;tt&gt;Fin&lt;/tt&gt;
or the non-regular datatypes reflecting the indices at the type level
we are going to use:&lt;/p&gt;



&lt;p class=code&gt;data Lam (A : Set) : Set where
  var : (a : A)             → Lam A
  app : (t u : Lam A)       → Lam A
  lam : (b : Lam (Maybe A)) → Lam A&lt;/p&gt;



&lt;p&gt;&lt;tt&gt;Lam&lt;/tt&gt; quite clearly is a functor hence the existence of
a higher-order function &lt;tt&gt;map&lt;/tt&gt; which loosely corresponds
to the notion of renaming / weakening &lt;a id=&quot;reftop2&quot; href=&quot;#refbot2&quot;&gt;[2]&lt;/a&gt;.
The &lt;tt&gt;var&lt;/tt&gt; and &lt;tt&gt;app&lt;/tt&gt; cases are unchanged but the type
of the lamdba&#39;s body now rejects the erroneous recursive call
described earlier. We are forced to both extend the substitution
with the new bound variable which is unchanged, and weaken the
elements in the rest of the substitution.&lt;/p&gt;



&lt;p class=code&gt;subst : (t : Lam A) (ρ : A → Lam B) → Lam B
subst (var a)   ρ = ρ a
subst (app t u) ρ = app (subst t ρ) (subst u ρ)
subst (lam b)   ρ = lam (subst b ρ&#39;)
  where ρ&#39; none     = var none
        ρ&#39; (some a) = &lt;span class=bad&gt;map some (ρ a)&lt;/span&gt; -- this traverses
                                     -- the whole term&lt;/p&gt;



&lt;p&gt;Unfortunately, this is rather inefficient in that we have to
walk through all the terms in the substitution every time we
pass under a binder.&lt;/p&gt;



&lt;h3&gt;&lt;a href=&quot;code/LazyLambda/LazyLambda.html#5159&quot;&gt;Lazy weakenings&lt;/a&gt;&lt;/h3&gt;



&lt;p&gt;In order to avoid traversing the whole term whenever one just
wants to weaken it, it is quite natural to add some redundancies
in the representation. This can be done by letting the variables
in a subterm be either bound by the nearest binder or &lt;i&gt;whole&lt;/i&gt;
subterms living in the non-extended context (aka weakened subterms).
This naturally translates to the following notions of
&lt;tt&gt;Scope&lt;/tt&gt;:&lt;/p&gt;



&lt;p class=code&gt;Scope : (F : Set → Set) (A : Set) → Set
Scope F A = F (Maybe (F A))&lt;/p&gt;



&lt;p&gt;In this case, the definition is simple enough that Agda has no
problem seeing that the &lt;tt&gt;Scope&lt;/tt&gt; transformer preserves
positivity and that &lt;tt&gt;Lam&lt;/tt&gt; is therefore a legal inductive
definition. However in more intricate examples, e.g. when describing
a universe of syntaxes with &lt;tt&gt;Scope&lt;/tt&gt;s, only a tool like
&lt;a href=&quot;http://www2.tcs.ifi.lmu.de/~abel/miniagda/&quot;&gt;MiniAgda&lt;/a&gt;
which allows for positivity annotations will accept the definition
as valid.&lt;/p&gt;



&lt;p class=code&gt;data Lam (A : Set) : Set where
  var : (a : A)           → Lam A
  app : (t u : Lam A)     → Lam A
  lam : (b : Scope Lam A) → Lam A&lt;/p&gt;



&lt;h3&gt;&lt;a href=&quot;code/LazyLambda/LazyLambda.html#7263&quot;&gt;Equality testing&lt;/a&gt;&lt;/h3&gt;



&lt;p&gt;All is well when it comes to substitution but what about
equality testing? We have added redundancies which therefore
need to be quotiented out when comparing two terms. The simplest
way to implement such a comparison function is to define a
function &lt;tt&gt;flatten&lt;/tt&gt; which will push the weakenings into the
leaves of the terms thus producing terms in &lt;i&gt;de Bruijn normal
form&lt;/i&gt;.&lt;/p&gt;



&lt;p class=code&gt;flatten : (t : Scope Lam A) → Lam (Maybe A)&lt;/p&gt;



&lt;h3&gt;&lt;a href=&quot;code/LazyLambda/LazyLambda.html#7932&quot;&gt;Lazy equality testing&lt;/a&gt;&lt;/h3&gt;



&lt;p&gt;Now, we know that we can test terms for equality. But it is not
quite satisfactory yet: we introduced redundancies precisely to avoid
having to perform all of these costly flattenings. But, with a little
bit of work, it is actually possible to perform &lt;i&gt;lazy&lt;/i&gt; equality
testing.&lt;/p&gt;



&lt;h4&gt;&lt;a href=&quot;code/LazyLambda/LazyLambda.html#4498&quot;&gt;Context inclusions&lt;/a&gt;&lt;/h4&gt;



&lt;p&gt;One of the tools we are going to need is a way to describe how
different contexts are related. The terms we are going to compare
will both live in a common context but unfolding them may
very-well bring us to widely different places based on the number
of weakenings respectively encountered on the way down to the
current subterms.&lt;/p&gt;



&lt;p&gt;We introduce this notion of inclusion &lt;a id=&quot;reftop3&quot; href=&quot;#refbot3&quot;&gt;[3]&lt;/a&gt;
whose 2nd constructor (&lt;tt&gt;↑_&lt;/tt&gt;) tags the moment one goes under
a lambda abstraction whilst the 3rd one (&lt;tt&gt;◂_&lt;/tt&gt;) tags the
moment one encounters a weakening. &lt;/p&gt;



&lt;p class=code&gt;data _⊆_ : (A B : Set) → Set₁ where
  ■  : A ⊆ A
  ↑_ : A ⊆ B → Maybe (Lam A) ⊆ Maybe (Lam B)
  ◂_ : Maybe (Lam A) ⊆ B → A ⊆ B&lt;/p&gt;



&lt;h4&gt;&lt;a href=&quot;code/LazyLambda/LazyLambda.html#10054&quot;&gt;Term comparison&lt;/a&gt;&lt;/h4&gt;



&lt;p&gt;Now, we are ready to describe the comparison of two terms living
in their respective local contexts &lt;tt&gt;A&lt;/tt&gt; and &lt;tt&gt;B&lt;/tt&gt; both
sharing a common super-context &lt;tt&gt;T&lt;/tt&gt;.&lt;/p&gt;



&lt;p class=code&gt;data EQ : {T A : Set} (incA : A ⊆ T) (t : Lam A)
          {B : Set}   (incB : B ⊆ T) (u : Lam B)
          → Set₁ where
&lt;/p&gt;


&lt;p&gt;Let&#39;s start off with the easy cases: two applications are equal
whenever their functions (resp. arguments) are equal ; and two
lambda abstractions are equal whenever their bodies are equal in
the extended contexts.&lt;/p&gt;


&lt;p class=code&gt;  EQapp     : EQ incA t₁ incB t₂ →
              EQ incA u₁ incB u₂ →
              EQ incA (app t₁ u₁) incB (app t₂ u₂)

  EQlam     : EQ (↑ incA) b₁ (↑ incB) b₂ →
              EQ incA (lam b₁) incB (lam b₂)&lt;/p&gt;


&lt;p&gt;When two terms claim to be bound variables, we can compare the
context inclusions to check that they refer to the same lambda
binder. We will describe what this check amounts to in the next
section.&lt;/p&gt;


&lt;p class=code&gt;  EQvar     : EQVar incA zero incB zero →
              EQ incA (var none) incB (var none)&lt;/p&gt;


&lt;p&gt;When one of the terms is a weakening of a subterm, we record this
information in the proof of context inclusion and keep on unfolding
further.&lt;/p&gt;


&lt;p class=code&gt;  EQvarFre₁ : EQ (◂ incA) v incB u →
              EQ incA (var (some v)) incB u

  EQvarFre₂ : EQ incA t (◂ incB) v →
              EQ incA t incB (var (some v))
&lt;/p&gt;



&lt;p&gt;The careful reader will have noticed that this description is
mostly syntax-directed except for the two last rules which happen
to commute. Turning this specification into a recursive algorithm
will therefore just be a matter of picking an order in which to
check that either &lt;tt&gt;EQvarFre₁&lt;/tt&gt; or &lt;tt&gt;EQvarFre₂&lt;/tt&gt; applies.
Which is good news when you are in the business of actually
deciding equality.&lt;/p&gt;




&lt;h4&gt;&lt;a href=&quot;code/LazyLambda/LazyLambda.html#8474&quot;&gt;Variable
comparison&lt;/a&gt;&lt;/h4&gt;



&lt;p&gt;Variable comparison is always done up to weakenings. We count
the numbers of weakenings encountered thus far respectively in
&lt;tt&gt;kA&lt;/tt&gt; and &lt;tt&gt;kB&lt;/tt&gt; and manage (in/de)crements based on
the tokens we stumble upon.&lt;/p&gt;



&lt;p class=code&gt;data EQVar : {TA A : Set} (incA : A ⊆ TA) (kA : Nat)
             {TB B : Set} (incB : B ⊆ TB) (kB : Nat)
             → Set₁ where&lt;/p&gt;


&lt;p&gt;When both context inclusions are synchronized on a lambda
binder (tagged by &lt;tt&gt;↑_&lt;/tt&gt; in the derivations) and no weakening
whatsoever has been applied (both &lt;tt&gt;kA&lt;/tt&gt; and &lt;tt&gt;kB&lt;/tt&gt; are
zero) then we can conclude that the variables are indeed equal.&lt;/p&gt;


&lt;p class=code&gt;  EQVarZRO : EQVar (↑ incA) zero (↑ incB) zero
&lt;/p&gt;



&lt;p&gt;When, on the other hand, they both claim to be referring to a
binder which is at least one level up (both &lt;tt&gt;kA&lt;/tt&gt; and
&lt;tt&gt;kB&lt;/tt&gt; are non-zero), one can forget about the binder at hand
and go explore the context inclusions upward.&lt;/p&gt;



&lt;p class=code&gt;  EQVarSUC : EQVar incA kA incB kB →
             EQVar (↑ incA) (suc kA) (↑ incB) (suc kB)&lt;/p&gt;


&lt;p&gt;Finally, when we encounter a weakening (marked with a
&lt;tt&gt;◂_&lt;/tt&gt;), we record its presence by incrementing the appropriate
counter and keep comparing the rest of the inclusion proofs.&lt;/p&gt;



&lt;p class=code&gt;  EQVarWK₁ : EQVar incA (suc kA) incB kB →
             EQVar (◂ incA) kA incB kB

  EQVarWK₂ : EQVar incA kA incB (suc kB) →
             EQVar incA kA (◂ incB) kB&lt;/p&gt;



&lt;h4&gt;&lt;a href=&quot;code/LazyLambda/LazyLambda.html#12026&quot;&gt;Testing equality&lt;/a&gt;&lt;/h4&gt;



&lt;p&gt;As expected, this specification also is mostly syntax-directed
except for commutative steps. It is thus easy to write an algorithm
checking whether such an equality derivation can be built. And it&#39;s
quite a lot simpler to do now that the rules are guiding the
implementation.&lt;/p&gt;

&lt;h3&gt;Footnotes&lt;/h3&gt;&lt;a id=&quot;refbot1&quot; href=&quot;#reftop1&quot;&gt;[1]&lt;/a&gt; &lt;a href=&quot;http://codereview.stackexchange.com/questions/44470/beta-reducer-in-haskell/44518#44518&quot;&gt;Weakening&lt;/a&gt;
is also a good candidate for this sort of nonsense given that
one easily forgets to protect bound variables and end up treating
them as if they were free.&lt;br /&gt;&lt;a id=&quot;refbot2&quot; href=&quot;#reftop2&quot;&gt;[2]&lt;/a&gt; It &lt;i&gt;is&lt;/i&gt;
respectively renaming / weakening provided that the function
passed is well-behaved (resp. bijective / injective).&lt;br /&gt;&lt;a id=&quot;refbot3&quot; href=&quot;#reftop3&quot;&gt;[3]&lt;/a&gt; It makes sense as
a notion of inclusion because a proof that &lt;tt&gt;A ⊆ B&lt;/tt&gt; gives rise
to a morphism from &lt;tt&gt;A&lt;/tt&gt; to &lt;tt&gt;B&lt;/tt&gt;. Cf.
&lt;a href=&quot;code/LazyLambda/LazyLambda.html#4861&quot;&gt;the accompanying Agda doc&lt;/a&gt;.</description><guid isPermaLink="true">http://www.gallais.org/blog/lazy-lambda.html</guid><pubDate>Mon, 28 Apr 2014 13:37:00 UTC</pubDate></item><item><title>Dimension-Aware Computations</title><link>http://www.gallais.org/blog/dimension-aware-computations.html</link><description>&lt;head&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot;       href=&quot;http://www.gallais.org/css/main.css&quot; /&gt;&lt;/head&gt;&lt;span style=&quot;float:right&quot;&gt;&lt;a href=&quot;/rss.xml&quot;&gt;&lt;img src=&quot;/img/rss.png&quot; /&gt;&lt;/a&gt;&lt;/span&gt;&lt;div class=bdocs&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;code/ToyingDimensions/dimensions.html&quot;&gt;Colored Agda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;code/ToyingDimensions/Plot.hs&quot;&gt;Haskell Voodoo&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;



&lt;p id=chapo&gt;Dimensional analysis is quite the tool to track down stupid
mistakes in physics much like a type-checker will detect non-sensical
expressions in your favourite statically-typed programming language.
I have been meaning to toy around with this notion in a dependently-typed
setting for quite a while. Here are a few definitions. As well as an
Agda program that needs to be &lt;i&gt;compiled&lt;/i&gt;.&lt;/p&gt;



&lt;p&gt;On Wednesday, &lt;a href=&quot;http://www.bentnib.org&quot;&gt;Bob&lt;/a&gt; presented at
&lt;a href=&quot;http://homepages.inf.ed.ac.uk/slindley/spls-2013-10/&quot;&gt;SPLS&lt;/a&gt;
a really cool
&lt;a href=&quot;http://bentnib.org/posts/2013-07-17-one-done-two-submitted.html&quot;&gt;
project&lt;/a&gt; he has been working on: a type system which, by parametricity,
derives automagically &lt;a id=&quot;reftop1&quot; href=&quot;#refbot1&quot;&gt;[1]&lt;/a&gt; the premises of
&lt;a href=&quot;https://en.wikipedia.org/wiki/Noether&#39;s_theorem&quot;&gt;Noether&#39;s
theorem&lt;/a&gt;. All this talking about physics got me motivated enough to
spend Saturday&#39;s afternoon hacking up a toy example. This is probably not
revolutionary &lt;a id=&quot;reftop2&quot; href=&quot;#refbot2&quot;&gt;[2]&lt;/a&gt; but it was quite
amusing to develop anyway so here it is.&lt;/p&gt;



&lt;h3&gt;The data structures involved&lt;/h3&gt;



&lt;p&gt;This whole project is structured as a succession of modules in order to
to have separate name spaces. This way, we can make sure that the different
operations morally implementing the same concepts can have the same name.&lt;/p&gt;



&lt;h4&gt;Dimensions&lt;/h4&gt;



&lt;p&gt;We will limit ourselves to 3 types of units of measure here: kilograms,
meters and seconds. The dimension of an object is modelled by a record storing
the exponents assigned to each one of these components.&lt;/p&gt;



&lt;p class=code&gt;record dimension : Set where
  field
    kilogram : ℤ
    meter    : ℤ
    second   : ℤ&lt;/p&gt;



&lt;p&gt;Taking the product of two dimensions amounts to summing the degrees for each
one of the units of measure. Quotienting is, likewise, a pointwise operation on
the two vectors: this time we compute the difference rather than the sum.&lt;/p&gt;



&lt;p class=code&gt;_*_ : (d e : dimension) → dimension
d * e = record { kilogram = kilogram d + kilogram e
               ; meter    = meter d    + meter e
               ; second   = second d   + second e }&lt;/p&gt;



&lt;p&gt;We can now define the basic dimensions: kilograms, meters and seconds by
assigning &lt;tt&gt;+ 0&lt;/tt&gt; to all the fields except for the one of interest
which is instantiated with &lt;tt&gt;+ 1&lt;/tt&gt;. As an example, here is the dimension
&lt;tt&gt;sec&lt;/tt&gt; corresponding to seconds.&lt;/p&gt;



&lt;p class=code&gt;sec = record { kilogram = + 0
             ; meter    = + 0
             ; second   = + 1 }&lt;/p&gt;



&lt;h4&gt;Units of measure&lt;/h4&gt;



&lt;p&gt;So far, so good. But in real life we quite like to consider other units than
just the basic ones. A &lt;tt&gt;unit&lt;/tt&gt; of measure is therefore a dimension
together with a non-zero coefficient. The type of &lt;tt&gt;hn&lt;/tt&gt; has been chosen
so &lt;tt&gt;hn&lt;/tt&gt; can be inferred by the system in concrete cases (if &lt;tt&gt;n&lt;/tt&gt;
is non-zero, then &lt;tt&gt;hn&lt;/tt&gt; is of type &lt;tt&gt;⊤&lt;/tt&gt; hence equal to &lt;tt&gt;tt&lt;/tt&gt;)
and to match the requirements imposed by &lt;tt&gt;_div_&lt;/tt&gt; in the standard
library.&lt;/p&gt;



&lt;p class=code&gt;data unit : Set where
  _,_#_ : (n : ℕ) (hn : False (n ℕ.≟ 0))
          (d : dimension) → unit&lt;/p&gt;



&lt;p&gt;The product of two units is once more a pointwise operations: it is the
combination of the product of their coefficients (which is guaranteed to be
non-zero) and the product of their respective dimensions.&lt;/p&gt;



&lt;p class=code&gt;_*_ : (u v : unit) → unit
(k , hk # d) * (l , hl # e) = k * l , _ # d * e&lt;/p&gt;



&lt;p&gt;It is quite natural to introduce the SI&#39;s prefixes as functions altering a
previously defined unit. In our system, the types of &lt;tt&gt;milli&lt;/tt&gt;,
&lt;tt&gt;centi&lt;/tt&gt; and &lt;tt&gt;deci&lt;/tt&gt; are nastier than the ones presented below
because we are using natural numbers rather than rational ones and division
of a non-zero number by a non-zero number can return zero.&lt;/p&gt;



&lt;p class=code&gt;deca hecto kilo : unit → unit&lt;/p&gt;



&lt;p&gt;A minute is nothing more than 60 seconds. 60 being different from 0 (!), the
proof obligation reduces to the only inhabitant of unit which is inferred by
the system:&lt;/p&gt;



&lt;p class=code&gt;min : unit
min = 60 , _ # sec&lt;/p&gt;



&lt;h4&gt;Values&lt;/h4&gt;



&lt;p&gt;Finally, values are just numerical values repackaged together with a unit.
We choose here to have a data constructor forcing the user to explicitly
mention the unit of measure to be attached to the value. It could be avoided
given that &lt;tt&gt;d&lt;/tt&gt; is a parameter of the data-type but it is quite handy
for documentation purposes and on the fly definition of values.&lt;/p&gt;



&lt;p class=code&gt;data \[_] : (d : unit) → Set where
  ⟨_∶_⟩ : (value : ℕ) (d : unit) → \\[ d ]&lt;/p&gt;



&lt;p&gt;Multiplying to value has to return something of the right dimension but the
scaling factor can be arbitrary (we may multiply &lt;tt&gt;kilo&lt;/tt&gt; meters by hours
and expect a result in &lt;tt&gt;m / s&lt;/tt&gt;). Hence the following implementation.
Addition and division behave similarly.&lt;/p&gt;



&lt;p class=code&gt;_*_ : \\[ k , hk #  d ] → \\[ l , hl # e ] →
      \\[ m , hm # d * e ]
_*_ ⟨ vd ∶ ._ ⟩ ⟨ ve ∶ ._ ⟩ =
    ⟨ (k * vd * l * ve) div m ∶ _ ⟩&lt;/p&gt;



&lt;p&gt;Our implementation of value multiplication is so generic in its type that
it does not have the subformula property which will probably be problematic
in large expression where the type of all the subexpressions is not inferrable.
A simple solution is to specialize the return type down to
&lt;tt&gt;\\[ 1 , _ # d * e ]&lt;/tt&gt; whilst introducing a lifting operation converting
from one type to an other.&lt;/p&gt;



&lt;p class=code&gt;↑ : ∀ {k hk l hl d} → \[ k , hk # d ] → \[ l , hl # d ]
↑ ⟨ v ∶ ._ ⟩ = ⟨ (k * v) div l ∶ _ ⟩&lt;/p&gt;



&lt;h3&gt;Examples&lt;/h3&gt;



&lt;p&gt;This system tracking dimensions makes sure that we do not make mistakes
when combining different values but it also converts between various units.
For instance, the following expression normalizes to &lt;tt&gt;10&lt;/tt&gt;.&lt;/p&gt;



&lt;p class=code&gt;60hm/min : \[ deca m / s ]
60hm/min = ⟨ 60 ∶ hecto m ⟩ / ⟨ 1 ∶ min ⟩&lt;/p&gt;



&lt;h3&gt;Application: simulating the free fall of a ball&lt;/h3&gt;



&lt;p&gt;The setting of this simulation is pretty simple: we have a ball
characterized by its position, speed and acceleration in a vertical plan.
The gravitational constant &lt;tt&gt;g&lt;/tt&gt; corresponding to earth&#39;s attraction
is expressed in &lt;tt&gt;m / (s * s)&lt;/tt&gt; and equal to &lt;tt&gt;10&lt;/tt&gt;.

&lt;br /&gt;In order to simulate the free fall of this ball, we are going to apply
Newton&#39;s approximation method: time is discretized and the characterization
of the ball is updated at every single time-step. Here the type system
guarantees that we do not make mistakes.&lt;/p&gt;



&lt;p class=code&gt;newton : ∀ (dt : \[ s ]) (p : point) → point
newton dt p =
  record { accx = accx p
         ; accy = accy p + g
         ; vx   = vx p   + (accx p :* dt)
         ; vy   = vy p   + (accy p :* dt)
         ; x    = x p    + (vx p   :* dt)
         ; y    = y p    + (vy p   :* dt) }&lt;/p&gt;



&lt;p&gt;A simulation consists in applying repeatedly this newton transformation
to an initial point. We define the &lt;tt&gt;throw&lt;/tt&gt; function doing exactly
this and generating a trace of all the successive positions.&lt;/p&gt;



&lt;p class=code&gt;throw : (n : ℕ) (dt : \[ s ]) → point → Vec point (ℕ.suc n)
throw zero    dt p = p ∷ \[]
throw (suc n) dt p = p ∷ throw n dt (newton dt p)&lt;/p&gt;



&lt;p&gt;After a little bit of magic, we are able to call a tracing function from
Haskell&#39;s &lt;a href=&quot;http://hackage.haskell.org/package/gnuplot-0.5.1&quot;&gt;Gnuplot&lt;/a&gt;
wrapper and generate the following graph showing the (upside-down) ball&#39;s
trajectory in the vertical plan.&lt;/p&gt;


&lt;center&gt;&lt;img src=&quot;/img/freefall.png&quot; /&gt;&lt;/center&gt;


&lt;h3&gt;Conclusions&lt;/h3&gt;



&lt;p&gt;First of all, it works quite well for a first experiment! In a serious
implementation though, one should really use, at the very least, rational
numbers to represent the coefficients. This would ensure that conversion
ratios can be properly computed and make the work a lot more useful.
It is no coincidence that my example involving a speed expressed in
&lt;i&gt;distance per minutes&lt;/i&gt; precisely picks a distance which is a multiple of
&lt;tt&gt;60&lt;/tt&gt;... I haven&#39;t had the internet connection, the time nor the
desire to see whether appropriate libraries already exist for Agda or to
write my own ones.
&lt;br /&gt;Similarly, the fact that the free fall is represented upside-down is a
direct consequence of the values not being integers but rather natural
numbers (once more due to a lack of libraries in Agda: &lt;tt&gt;div&lt;/tt&gt; and
&lt;tt&gt;mod&lt;/tt&gt; are only available for ℕ in the standard library).&lt;/p&gt;



&lt;p&gt;And Agda&#39;s automatic case-splitting, support for Unicode, definition of
functions &lt;i&gt;via&lt;/i&gt; equations, my (ridiculously small but good enough)
understanding of how to import Haskell function as postulates, etc. kept me
away from Coq (at least for the experimentation phase).&lt;/p&gt;

&lt;h3&gt;Footnotes&lt;/h3&gt;&lt;a id=&quot;refbot1&quot; href=&quot;#reftop1&quot;&gt;[1]&lt;/a&gt; A good start to learn about the free lunch
provided by parametricity is obviously Phil Wadler&#39;s &lt;i&gt;Theorems for Free!&lt;/i&gt;
(&lt;a href=&quot;http://www.citeulike.org/user/gallais/article/1848264&quot;&gt;citeulike&lt;/a&gt;).
&lt;br /&gt;&lt;a id=&quot;refbot2&quot; href=&quot;#reftop2&quot;&gt;[2]&lt;/a&gt; Andrew Kennedy&#39;s paper &lt;i&gt;Types for
Units-of-Measure: Theory and Practice&lt;/i&gt; mentions that abusing C++ or
Haskell&#39;s type systems makes it possible to « achieve it, but at some cost
in usability ». Arguably, a real dependently-typed language is more suited
for computing at the type level in this fashion.</description><guid isPermaLink="true">http://www.gallais.org/blog/dimension-aware-computations.html</guid><pubDate>Sat,  2 Nov 2013 13:37:00 UTC</pubDate></item><item><title>Glueing terms to models</title><link>http://www.gallais.org/blog/glueing-terms-models.html</link><description>&lt;head&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot;       href=&quot;http://www.gallais.org/css/main.css&quot; /&gt;&lt;/head&gt;&lt;span style=&quot;float:right&quot;&gt;&lt;a href=&quot;/rss.xml&quot;&gt;&lt;img src=&quot;/img/rss.png&quot; /&gt;&lt;/a&gt;&lt;/span&gt;&lt;div class=bdocs&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://patch-tag.com/r/gallais/agda/snapshot/current/content/raw/src/nbe/gluedmodel.agda&quot;&gt;Raw Agda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/code/GlueingModel/gluedmodel.html&quot;&gt;Colored Agda&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;


&lt;p id=chapo&gt;Today I want to talk about something I came up with when
working on questions connected to the material presented in our
&lt;a href=&quot;/pdf/icfp13.pdf&quot;&gt;ICFP submission&lt;/a&gt; but we did not get the
opportunity to discuss in the paper. The claim is that glueing
terms to the structure of a model allows the reader to get a good
intuition of the way the calculus works by making explicit what used
to be hidden &lt;i&gt;under the hood&lt;/i&gt;.&lt;/p&gt;



&lt;p&gt;I learned about the glueing technique whilst reading about
models of the simply-typed SK combinatorial calculus
&lt;a id=&quot;reftop1&quot; href=&quot;#refbot1&quot;&gt;[1]&lt;/a&gt;. Glueing is described as a tool to
circumvent the mismatch between the really restrictive structure of
SK terms and the rather liberal (naïve) model: if one is able to
evaluate SK terms in the model, it is however impossible to extract
normal forms from the semantical objects without applying some kind of lambda to SK encoding procedure! Glueing syntactical terms
to their semantical counterparts is precisely what is required by
the procedure to be able to reify partial applications of S and K.&lt;/p&gt;



&lt;p&gt;Glueing resurfaced later on in my own experimentations when
defining a model for a weak-head normalization by evaluation
&lt;a id=&quot;reftop2&quot; href=&quot;#refbot2&quot;&gt;[2]&lt;/a&gt;.
Indeed, the need to be able to throw away unnecessary computations
(e.g. the argument part of a stuck application should not be
reduced) is easily met by enriching the model with syntactical
artefacts corresponding to &lt;i&gt;source terms&lt;/i&gt; for the semantical
objects. A pleasant side-effet of the added structure is the
simplification of the formulation and proofs of correctness
properties in Type Theory.&lt;/p&gt;



&lt;h3&gt;The setting&lt;/h3&gt;



&lt;p&gt;But in this blog post I would like to argue that there is more
to it than merely solving these kind of problems: glueing can be
used to obtain a well-structured model highlighting some nice
properties of the calculus. The setting for this example is the
definition of a normalization function for the simply-typed lambda
calculus which does not require a glueing to be defined. Types are
either the base, uninterpreted, type or arrow types:&lt;/p&gt;



&lt;p class=code&gt;σ, τ : ty ∷= ♭ | σ `→ τ&lt;/p&gt;



&lt;p&gt;Our well-typed terms using de Bruijn indices are boring so we
will have a look at the definition of normal forms instead. By
definition, only stuck expressions (a variable followed by a spine
of arguments in normal form) of the base type can be regarded as
normal forms. The equational theory will accordingly include eta
rules.&lt;/p&gt;



&lt;p class=code&gt;  data _⊢ne_ (Γ : Con ty) : ty → Set where
    `v   : (pr : σ ∈ Γ)                     → Γ ⊢ne σ
    _`$_ : (f : Γ ⊢ne σ `→ τ) (x : Γ ⊢nf σ) → Γ ⊢ne τ

  data _⊢nf_ (Γ : Con ty) : ty → Set where
    `↑_ : (t : Γ ⊢ne ♭)     → Γ ⊢nf ♭
    `λ_ : (b : Γ ∙ σ ⊢nf τ) → Γ ⊢nf σ `→ τ&lt;/p&gt;



&lt;p&gt;All these notions obviously come with the expected weakening
operations.&lt;/p&gt;



&lt;h3&gt;Model definition - where the glueing happens&lt;/h3&gt;



&lt;p&gt;The model is defined in terms of reduction-free elements: the
interpretation of a term is either a neutral form which will just
grow when being eliminated or it is a normal form together with
an element of the &lt;i&gt;acting&lt;/i&gt; model explaining what its behaviour
is.&lt;/p&gt;



&lt;p class=code&gt;  _⊩_ : ∀ (Γ : Con ty) (σ : ty) → Set
  Γ ⊩ σ =   Γ ⊢ne σ
          ⊎ Γ ⊢nf σ × Γ ⊩⋆ σ&lt;/p&gt;



&lt;p&gt;The acting model is the part of the model doing all the heavy
lifting when &lt;i&gt;computational&lt;/i&gt; reductions are required. It has
a Kripke flavour in the sense that it refers to the full model in
any possible future context extension. Quite unsurprisingly, a
term of base type has no computational content (it can only be
a neutral) and elements of function type are interpreted as
functions on semantical objects.&lt;/p&gt;



&lt;p class=code&gt;  _⊩⋆_ : ∀ (Γ : Con ty) (σ : ty) → Set
  Γ ⊩⋆ ♭      = ⊤
  Γ ⊩⋆ σ `→ τ = ∀ {Δ} (inc : Γ ⊆ Δ) → Δ ⊩ σ → Δ ⊩ τ&lt;/p&gt;



&lt;p&gt;These definitions are easily extended to contexts by recursion
and give rise to semantical environments &lt;tt&gt;Δ ⊩ε Γ&lt;/tt&gt;. A natural
notion of weakening can be made formal.&lt;/p&gt;



&lt;h3&gt;The (not so) trivial quoting function&lt;/h3&gt;



&lt;p&gt;Because of the way the model is defined, there is little to no
mystery that proving that each element has an image in the model
amounts to proving the existence of a procedure turning terms into
their cut-free equivalents.&lt;/p&gt;



&lt;p&gt;Even if the definitions of quote and unquote are straightforward,
they are rather unusual: unlike their more traditional analogues, they
confine the uses of eta-expansion at reification time. This has to be
compared to the motto &lt;i&gt;eta-expansion both at the syntactical and
semantical level&lt;/i&gt; usually associated with the (un)quote functions.
Additionally, this syntactic work is performed in a standalone
function &lt;tt&gt;eta[_]_&lt;/tt&gt;
&lt;a id=&quot;reftop3&quot; href=&quot;#refbot3&quot;&gt;[3]&lt;/a&gt; which
is model agnostic.&lt;/p&gt;



&lt;p class=code&gt;eta[_]_ : ∀ {Γ} σ (T : Γ ⊢ne σ) → Γ ⊢nf σ
eta[ ♭      ] t = `↑ t
eta[ σ `→ τ ] t = `λ eta[ τ ] (wk-ne inc t `$ var)
  where inc = step (same _)
        var = eta[ σ ] `v here!&lt;/p&gt;



&lt;p&gt;This way of organizing extraction of normal forms from the model
brings us closer to a staged reduction process which deals
with computations (beta, delta, iota) first and then reorganizes
the cut-free forms using extra rules (eta, nu &lt;a id=&quot;reftop4&quot; href=&quot;#refbot4&quot;&gt;[4]&lt;/a&gt;) than your
average normalization by evaluation formalization.&lt;/p&gt;



&lt;h3&gt;Evaluation function&lt;/h3&gt;



&lt;p&gt;By combining weakening for semantical environments and unquote,
we can define a diagonal environment &lt;tt&gt;Γ ⊩ε Γ&lt;/tt&gt; for every
&lt;tt&gt;Γ&lt;/tt&gt;. In order to be able to extract normal forms from
simple terms, it is thus enough to define an evaluation function.&lt;/p&gt;



&lt;p class=code&gt;eval : (t : Γ ⊢ σ) (ρ : Δ ⊩ε Γ) → Δ ⊩ σ&lt;/p&gt;



&lt;p&gt;&lt;tt&gt;eval&lt;/tt&gt; is defined by induction on the structure of the term
&lt;tt&gt;t&lt;/tt&gt;. In the variable case, a simple lookup in the semantical
environment produces the well-typed value needed. The application
case combines the induction hypotheses using &lt;tt&gt;_$$_&lt;/tt&gt;, a
function defined by case analysis on the function: if the function
is stuck, the application also is whereas if it is live, the action
model will compute further using the argument.&lt;/p&gt;



&lt;p class=code&gt;_$$_ : (F : Γ ⊩ σ `→ τ) (X : Γ ⊩ σ) → Γ ⊩ τ
inj₁ f       $$ X = inj₁ (f `$ ↑[ _ ] X)
inj₂ (f , F) $$ X = F (⊆-refl _) X&lt;/p&gt;



&lt;p&gt;The lambda abstraction case is defined in two steps: first the
object &lt;tt&gt;B&lt;/tt&gt; in the acting model can be generated by applying
the induction hypothesis for the body of the lambda in an extended
(and weakened) environment. And then the normal form is generated
by quoting the object obtained when applying &lt;tt&gt;B&lt;/tt&gt; to the
variable bound by the head lambda.&lt;/p&gt;


&lt;p class=code&gt;eval (`λ t) ρ = inj₂ (`λ ↑B , B)
  where B  : _ ⊩⋆ _ `→ _
        B  = λ inc X → eval t (wk-⊩ε _ inc ρ , X)
        ↑B = ↑[ _ ] B (step (⊆-refl _)) (↓[ _ ] `v here!)&lt;/p&gt;



&lt;p&gt;Now we can just tie the knot and combine the diagonal environment
with the evaluation function and quote in order to obtain a
normalization procedure.&lt;/p&gt;



&lt;p class=code&gt;norm[_]_ : ∀ {Γ} σ (t : Γ ⊢ σ) → Γ ⊢nf σ
norm[ σ ] t = ↑[ σ ] eval t (⊩ε-refl _)&lt;/p&gt;



&lt;h3&gt;Conclusions&lt;/h3&gt;



&lt;p&gt;We recalled that glueing can help us give more structure to our
models thus circumventing limitations of the naïve ones. But, more
importantly, we have also shown that this technique can produce new
insights on already well understood constructions by e.g. making it
possible to isolate different stages of the normalization process:
computation and standardization in our case.&lt;/p&gt;

&lt;h3&gt;Footnotes&lt;/h3&gt;&lt;a id=&quot;refbot1&quot; href=&quot;#reftop1&quot;&gt;[1]&lt;/a&gt; Intuitionistic model constructions and normalization
proofs by Thierry Coquand and Peter Dybjer
(&lt;a href=&quot;http://www.citeulike.org/user/gallais/article/8927583&quot;&gt;citeulike&lt;/a&gt;)&lt;br /&gt;&lt;a id=&quot;refbot2&quot; href=&quot;#reftop2&quot;&gt;[2]&lt;/a&gt; &lt;a href=&quot;http://patch-tag.com/r/gallais/agda/snapshot/current/content/raw/src/nbe/stlc/whnf/model.agda&quot;&gt;Raw Agda&lt;/a&gt; development on patch-tag.&lt;br /&gt;&lt;a id=&quot;refbot3&quot; href=&quot;#reftop3&quot;&gt;[3]&lt;/a&gt; It should be noted that the traditional eta-expansion trick
(&lt;tt&gt;↑[ σ ] ↓[ σ ] t&lt;/tt&gt;) consisting of drowning the neutral term
in the model and refying it back immediately after is precisely
equal to applying &lt;tt&gt;eta[ σ ]&lt;/tt&gt; to &lt;tt&gt;t&lt;/tt&gt;.&lt;br /&gt;&lt;a id=&quot;refbot4&quot; href=&quot;#reftop4&quot;&gt;[4]&lt;/a&gt; A set of
rules reorganizing stuck recursive functions we introduce in our
&lt;a href=&quot;/pdf/icfp13.pdf&quot;&gt;ICFP submission&lt;/a&gt; (This is starting to
sound like an advertisement campaign... ¬_¬).</description><guid isPermaLink="true">http://www.gallais.org/blog/glueing-terms-models.html</guid><pubDate>Tue, 30 Apr 2013 13:37:00 UTC</pubDate></item><item><title>A universe for syntax with binding</title><link>http://www.gallais.org/blog/syntax-binding-run-omega.html</link><description>&lt;head&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot;       href=&quot;http://www.gallais.org/css/main.css&quot; /&gt;&lt;/head&gt;&lt;span style=&quot;float:right&quot;&gt;&lt;a href=&quot;/rss.xml&quot;&gt;&lt;img src=&quot;/img/rss.png&quot; /&gt;&lt;/a&gt;&lt;/span&gt;&lt;div class=bdocs&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://patch-tag.com/r/gallais/agda/snapshot/current/content/raw/src/generic/unindexed.agda&quot;&gt;Raw Agda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;code/SyntaxWithBinding/unindexed.html&quot;&gt;Colored Agda&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;



&lt;p id=chapo&gt;Generic programming has proven useful to define automatically operations
acting on datatypes &lt;i&gt;à la&lt;/i&gt; deriving in Haskell. However usual presentations
do not allow the user to highlight some constructors as binding variables in the
context. For every language with binders defined in such a universe, it is therefore
necessary to define manually substitution and to prove its properties.

&lt;br /&gt;In this blog post, we strive to define a universe for syntaxes with binding
for which substitution can be defined generically.&lt;/p&gt;



&lt;p&gt;Operations such as maps, induction principles, equality tests, etc. acting on
inductive datatypes can be defined generically over datatypes inhabiting a universe.
Altenkirch and McBride &lt;a id=&quot;reftop1&quot; href=&quot;#refbot1&quot;&gt;[1]&lt;/a&gt; have shown how
dependently-typed systems can accomodate these mechanisms as a set of libraries
rather than e.g. a pre-processor.&lt;/p&gt;



&lt;h3&gt;The universe&lt;/h3&gt;



&lt;p&gt;Our set of description constructors is basically identical to the one of McBride
in his paper on ornaments &lt;a id=&quot;reftop2&quot; href=&quot;#refbot2&quot;&gt;[2]&lt;/a&gt;
except for an extra constructor which allows us to introduce a new variable. It
comes with a decoding function which, for this is the non dependent case, can be
defined separately. We will however present each constructor together with its
semantics given by the decoding function &lt;tt&gt;⟦_⟧&lt;/tt&gt; in the pure tradition of
Induction-Recursion &lt;a id=&quot;reftop3&quot; href=&quot;#refbot3&quot;&gt;[3]&lt;/a&gt;. This decoding function defines an endofunctor on &lt;tt&gt;List V → Set&lt;/tt&gt;.&lt;/p&gt;



&lt;p class=code&gt;data Desc : Set₁ where
⟦_⟧ : ∀ {V} → Desc → (R : List V → Set) → List V → Set&lt;/p&gt;



&lt;p&gt;Sigma types can either be used in a non-dependent fashion in order to store
elements of a set inside the structure of the datatype e.g. when defining lists;
or to give the user a set (indexed by &lt;tt&gt;A&lt;/tt&gt; of different constructors she may
use.&lt;/p&gt;



&lt;p class=code&gt;`σ[_]_ : (A : Set) → (da : (a : A) → Desc) → Desc
⟦ `σ[ A ] d ⟧ R Γ = Σ A (λ a → ⟦ d a ⟧ R Γ)&lt;/p&gt;



&lt;p&gt;Recursive positions are the variable positions in the functor and are therefore
interpreted by &lt;tt&gt;R&lt;/tt&gt; which will be the fixpoint itself when tying the knot.
They correspond to e.g. the tail of the list in the description of the &lt;tt&gt;_∷_&lt;/tt&gt;
constructor.&lt;/p&gt;



&lt;p class=code&gt;`r_ : (d : Desc) → Desc
⟦ `r d ⟧ R Γ = R Γ × ⟦ d ⟧ R Γ&lt;/p&gt;



&lt;p&gt;Units are used to state that a description is finished. In the indexed case,
they will be tagged with an index restricting the type of the branch they belong
to.&lt;/p&gt;



&lt;p class=code&gt;`1 : Desc
⟦ `1 ⟧ R Γ = ⊤&lt;/p&gt;



&lt;p&gt;Binders expect the user to provide an identifier for the bound variable and
then keep going through the reste of the definition with an extended context.
In this post, we never use informative identifiers but we can easily imagine
taking advantage of a set with decidable equality to alleviate the burden of
calling a variable by providing a proof that it belongs to an environment.&lt;/p&gt;



&lt;p class=code&gt;`λ_ : (d : Desc) → Desc
⟦ `λ d ⟧ R Γ = Σ _ (λ v → ⟦ d ⟧ R (v ∷ Γ))&lt;/p&gt;



&lt;p&gt;So far so good but... we have no way to actually use a variable! Sure, we can
declare as many new ones as we fancy and have the joy of seeing them floating
around in the context, but we might want to actually do something with them at
some point.&lt;/p&gt;



&lt;h4&gt;What is a variable?&lt;/h4&gt;



&lt;p&gt;In our universe with binders, we want to have a substitution operation
generically defined on all the datatypes using binding. In other words, a
variable&#39;s only purpose is to be substituted by a &quot;proper&quot; term. In a sense,
a variable is just a placeholder for a &quot;real&quot; term, an excuse for an inhabitant
which has not been crafted yet.
&lt;br /&gt;This is the reason why variables are treated separately: a whole subterm and
a variable should be on an equal footing. This ascertainment induces us to enrich
the meaning endofunctor with a position for variables right before building its
fixpoint.&lt;/p&gt;



&lt;p class=code&gt;L⟦_⟧ : ∀ {V} → Desc → (R : List V → Set) → List V → Set
L⟦ d ⟧ R Γ = Σ _ (λ v → v ∈ Γ) ⊎ ⟦ d ⟧ R Γ

data `μ {V} (d : Desc) (Γ : List V) : Set where
  `⟨_⟩ : (t : L⟦ d ⟧ (`μ d) Γ) → `μ d Γ&lt;/p&gt;



&lt;p&gt;Obviously Agda does not complain about our definition: the described functors
are strictly positive by construction and accordingly have a fixpoint.&lt;/p&gt;



&lt;h3&gt;Defining substitution&lt;/h3&gt;



&lt;p&gt;We call substitution for &lt;tt&gt;d&lt;/tt&gt; from &lt;tt&gt;Γ&lt;/tt&gt; to &lt;tt&gt;Δ&lt;/tt&gt;, a function
which provides for each variable in &lt;tt&gt;Γ&lt;/tt&gt; a term of type &lt;tt&gt;`μ d&lt;/tt&gt;
using variables taken in &lt;tt&gt;Δ&lt;/tt&gt;.&lt;/p&gt;



&lt;p class=code&gt;subst[_] : ∀ {V} → Desc → List V → List V → Set
subst[ d ] Γ Δ = ∀ v → v ∈ Γ → `μ d Δ&lt;/p&gt;



&lt;p&gt;As one would expect from the name of this function, given a substitution for
&lt;tt&gt;d&lt;/tt&gt; from &lt;tt&gt;Γ&lt;/tt&gt; to &lt;tt&gt;Δ&lt;/tt&gt; and a term in &lt;tt&gt;`μ d&lt;/tt&gt; with free
variables ranging in &lt;tt&gt;Γ&lt;/tt&gt;, one can build a term in &lt;tt&gt;`μ d&lt;/tt&gt; only using
variables in &lt;tt&gt;Δ&lt;/tt&gt;. The whole process corresponds to applying this substitution
and is defined by two mutually defined functions.
&lt;br /&gt;A term is either a variable, in which case we can just apply the substitution,
or it is an inhabitant of the functor applied to the fixpoint and we rely on the
helper function to propagate calls to the substitution on recursive positions.&lt;/p&gt;



&lt;p class=code&gt;mutual
  _∶_⇇_ : ∀ {V} {Γ Δ : List V} d (t : `μ d Γ)
    (ρ : subst[ d ] Γ Δ) → `μ d Δ
  d ∶ `⟨ inj₁ (v , pr) ⟩ ⇇ ρ = ρ v pr
  d ∶ `⟨ inj₂ t        ⟩ ⇇ ρ = `⟨ inj₂ (map⟦ d ⟧ t ρ) ⟩&lt;/p&gt;



&lt;p&gt;The helper function is quite straightforward to define except for the case
where we go under a binder. In this case, we extend the substitution with the
identity on the first variable and weaken the terms corresponding to the other
ones. The careful reader will notice the generalization applied to the type of
the recursive positions to ensure that the definition goes through. In practice,
all direct calls to &lt;tt&gt;map⟦_⟧&lt;/tt&gt; will be such that &lt;tt&gt;d = e&lt;/tt&gt;.&lt;/p&gt;



&lt;p class=code&gt;  map⟦_⟧ : ∀ {V} d {e} {Γ Δ : List V} (t : ⟦ d ⟧ (`μ e) Γ)
    (ρ : subst[ e ] Γ Δ) → ⟦ d ⟧ (`μ e) Δ
  map⟦ `σ[ A ] d ⟧ (a , t) ρ = a         , map⟦ d a ⟧ t ρ
  map⟦ `r d      ⟧ (r , t) ρ = _ ∶ r ⇇ ρ , map⟦ d ⟧ t ρ
  map⟦ `1        ⟧ t       ρ = t
  map⟦ `λ d      ⟧ (v , t) ρ = v , map⟦ d ⟧ t ρ&#39;
    where ρ&#39; = λ { .v z      → `⟨ inj₁ (v , z) ⟩ ;
                    w (s pr) → weaken _ _ (ρ w pr) }&lt;/p&gt;



&lt;p&gt;Given an easy proof relying on weakening that the identity substitution exists,
it is rather easy to define what&#39;s named β reduction in the lambda calculus.&lt;/p&gt;



&lt;p class=code&gt;βred : ∀ {V} d {Γ v} → `μ d (v ∷ Γ) → `μ d Γ → `μ d Γ
βred d {Γ} {v} t u = d ∶ t ⇇ ρ
  where ρ = λ { .v z      → u ;
                 w (s pr) → subst-id d Γ w pr }&lt;/p&gt;



&lt;h3&gt;Examples&lt;/h3&gt;



&lt;p&gt;When building examples, we introduce enumerations of labels corresponding to
the different constructors of the language. We hope the definitions of the various
&lt;tt&gt;Desc&lt;/tt&gt; functors to be easier to understand this way.
&lt;br /&gt;Let&#39;s start with a very simple language: the untyped lambda calculus. Its
grammar has two components: one can introduce a lambda abstraction (label
&lt;tt&gt;lam&lt;/tt&gt;) which binds a variable and contains a subterm (a recursive position
&lt;tt&gt;`r&lt;/tt&gt;) or an applications (label &lt;tt&gt;app&lt;/tt&gt;) which contains two subterms.
The set of lambda terms with variables in &lt;tt&gt;Γ&lt;/tt&gt; is then given by &lt;tt&gt;`μ&lt;/tt&gt;.
The set of closed terms is therefore &lt;tt&gt;lc []&lt;/tt&gt;&lt;/p&gt;



&lt;p class=code&gt;lc-grammar : Desc
lc-grammar =
  `σ[ lc-labl ] λ { lam → `λ `r `1 ;
                    app → `r `r `1 }

lc : ∀ (Γ : List ⊤) → Set
lc Γ = `μ lc-grammar Γ&lt;/p&gt;



&lt;p&gt;Using Andjelkovic and Gundry&#39;s pattern synonyms &lt;a id=&quot;reftop4&quot; href=&quot;#refbot4&quot;&gt;[4]&lt;/a&gt;, we can recover a nice syntax closer to the one we are
used to. Our lambdas, variables and applications will desugar to the somewhat
horrendous codes describing elements of the fixpoint of &lt;tt&gt;lc-grammar&lt;/tt&gt;.
We can, for instance, define the well-known looping function &lt;tt&gt;:Ω&lt;/tt&gt;.&lt;/p&gt;



&lt;p class=code&gt;:δ :Ω : lc []
:δ = :λ :v z :$ :v z
:Ω = :δ :$ :δ&lt;/p&gt;



&lt;p&gt;We can now define a function &lt;tt&gt;fire : lc [] → Maybe (lc [])&lt;/tt&gt; which explores
a (closed) term looking for a redex and returns either nothing if the term is in
weak-head normal form or just the term after having fired the first redex encountered.
Iterating &lt;tt&gt;fire&lt;/tt&gt; on a term gives rise to an &lt;tt&gt;execute&lt;/tt&gt; function
which produces a &lt;tt&gt;CoList&lt;/tt&gt; of successive reducts.&lt;/p&gt;



&lt;p&gt;As one would expect, running &lt;tt&gt;:Ω&lt;/tt&gt; yields an infinitely long &lt;tt&gt;CoList&lt;/tt&gt;
of &lt;tt&gt;:Ω&lt;/tt&gt;s.&lt;/p&gt;



&lt;p class=code&gt;lemma : execute :Ω ≣ repeat :Ω
lemma = ∷ ♯ lemma&lt;/p&gt;



&lt;h3&gt;What&#39;s next?&lt;/h3&gt;



&lt;p&gt;I cooked up the &lt;a href=&quot;http://patch-tag.com/r/gallais/agda/snapshot/current/content/raw/src/generic/indexed.agda&quot;&gt;indexed version&lt;/a&gt;
with which it is possible to get sexy well-typed terms for the simply-typed lambda
calculus such as &lt;tt&gt;`K&lt;/tt&gt;. But that is another story.&lt;/p&gt;



&lt;p class=code&gt;`K : ∀ {Γ σ τ} → Γ ⊢ σ `→ τ `→ σ
`K = :λ :λ :v (s z)&lt;/p&gt;

&lt;h3&gt;Footnotes&lt;/h3&gt;&lt;a id=&quot;refbot1&quot; href=&quot;#reftop1&quot;&gt;[1]&lt;/a&gt; &lt;a href=&quot;http://www.citeulike.org/user/gallais/article/6926730&quot;&gt;
Generic Programming within Dependently Typed Programming&lt;/a&gt;&lt;br /&gt;&lt;a id=&quot;refbot2&quot; href=&quot;#reftop2&quot;&gt;[2]&lt;/a&gt; Ornamental algebras, algebraic ornaments
(&lt;a href=&quot;https://personal.cis.strath.ac.uk/conor.mcbride/pub/OAAO/Ornament.pdf&quot;&gt;pdf&lt;/a&gt;)&lt;br /&gt;&lt;a id=&quot;refbot3&quot; href=&quot;#reftop3&quot;&gt;[3]&lt;/a&gt; Cf. e.g. Peter Dybjer&#39;s &lt;a href=&quot;http://www.cse.chalmers.se/~peterd/papers/inductive.html&quot;&gt;
work&lt;/a&gt;.&lt;br /&gt;&lt;a id=&quot;refbot4&quot; href=&quot;#reftop4&quot;&gt;[4]&lt;/a&gt; 
&lt;a href=&quot;http://code.google.com/p/agda/issues/detail?id=495&quot;&gt;Feature request&lt;/a&gt;
on agda&#39;s bug tracker.</description><guid isPermaLink="true">http://www.gallais.org/blog/syntax-binding-run-omega.html</guid><pubDate>Thu, 11 Apr 2013 13:37:00 UTC</pubDate></item></channel></rss>